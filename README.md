CodeT5 Docstring Generation with LoRA
This project fine-tunes a CodeT5-base model using LoRA to automatically generate Python docstrings from code. It includes scripts for data preparation, model training, and inference.

Prerequisites
Python 3.8+

pip

(Windows Users) WSL2 is recommended for better performance and GPU access.

Installation
Clone the repository:

Bash

git clone https://github.com/adrianbout/CodeT5_Docstring_Generation_with_LoRA.git
cd CodeT5_Docstring_Generation_with_LoRA
Create and Activate a Virtual Environment:

This is highly recommended to manage project dependencies.

Inside WSL2 (Linux environment):

Bash

python3 -m venv venv
source venv/bin/activate
On Windows (PowerShell/CMD, if not using WSL2):

PowerShell

python -m venv venv
.\venv\Scripts\activate
Install dependencies:

Bash

pip install -r requirements.txt
Download Dataset:

Download python.zip from the CodeSearchNet (Python) Kaggle dataset.

Create a data/ folder in the project root. (if data folder does not exist )

Place python.zip inside data/.

Usage
Follow these steps in order:

1. Data Preparation (data_cleaning.py)
This script unzips the raw dataset, cleans code-docstring pairs (e.g., removing duplicates, filtering by length), and splits them into train_processed.jsonl, valid_processed.jsonl, and test_processed.jsonl in the data/processedData/ directory.

Bash

python data_cleaning.py
2. Model Fine-tuning (training.py)
This script fine-tunes the Salesforce/codet5-base model using LoRA on the processed data. The fine-tuned model (LoRA adapters and tokenizer) will be saved to the ./codet5_base_fine_tuned_docstrings_lora directory.

Bash

python training.py
3. Docstring Generation (tester.py)
This script loads the fine-tuned LoRA model and generates docstrings for predefined sample Python code snippets.

Using your trained model:
The tester.py script is set up to load the model directly from the output path of training.py (./codet5_base_fine_tuned_docstrings_lora).

Using pre-trained weights (if you don't want to train):
If you have pre-trained model weights (LoRA adapters and tokenizer files) in a directory named FINETUNED_MODEL (or any other specified path), you can use them directly.
To do this, open tester.py and uncomment the line:
FINETUNED_MODEL_PATH = "./FINETUNED_MODEL"
and comment out:
FINETUNED_MODEL_PATH = "./codet5_base_fine_tuned_docstrings_lora"
Ensure the FINETUNED_MODEL directory exists and contains the necessary model files.

Bash

python tester.py

Challenges Faced
During the development of this project, several challenges were encountered:

Resource Constraints: Initial attempts to train on Google Colab were hindered by insufficient RAM memory and limited compute credits, necessitating a switch to local training.

Hardware Adaptation: The project was successfully moved to local training leveraging an NVIDIA RTX 3070 GPU.

Model Selection: Faced with numerous pre-trained models, CodeT5 was ultimately chosen for its smaller size and its specific pre-training on code-related tasks, making it a highly suitable base model.

Training Efficiency: To mitigate long training times and high memory usage, the LoRA (Low-Rank Adaptation) technique was implemented, effectively reducing the training duration by half.

Data Cleaning Complexity: The most challenging and arguably most crucial aspect of the project was the data cleaning phase. Ensuring high-quality, relevant, and properly formatted data was essential for effective model training.

Results
The fine-tuning process successfully reduced the evaluation loss from 0.39 to 0.18 over 10 epochs. Concurrently, ROUGE-1 scores significantly improved from approximately 10.5 to 28.8, indicating enhanced docstring generation quality.

Here's a snippet from the training logs demonstrating the performance improvement:

Starting fine-tuning with CodeT5 base with LoRA...
{'eval_loss': 0.3942526578903198, 'eval_rouge1': 10.4922, 'eval_rouge2': 1.4521, 'eval_rougeL': 9.8706, 'eval_rougeLsum': 9.6616, 'eval_gen_len': 15.332, 'eval_runtime': 43.6678, 'eval_samples_per_second': 5.725, 'eval_steps_per_second': 1.443, 'epoch': 1.0}

... (intermediate epochs omitted) ...

{'eval_loss': 0.15788649022579193, 'eval_rouge1': 33.6806, 'eval_rouge2': 12.5826, 'eval_rougeL': 32.3966, 'eval_rougeLsum': 32.5203, 'eval_gen_len': 7.66, 'eval_runtime': 29.3369, 'eval_samples_per_second': 8.522, 'eval_steps_per_second': 2.147, 'epoch': 10.0}


Below are some example results generated by the tester.py script. The input to the model is the code, and the output is the generated docstring.

--- Original Code 1 ---
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
--- Generated Docstring 1 ---
Compute the factorial of the given number.

--- Original Code 2 ---
def calculate_average(numbers):
    if not numbers:
        return 0
    return sum(numbers) / len(numbers)
--- Generated Docstring 2 ---
Calculate average of a list of numbers.

--- Original Code 3 ---
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
--- Generated Docstring 3 ---
Quick sort of an array.

--- Original Code 4 ---
def is_prime(num):
    if num < 2:
        return False
    for i in range(2, int(num**0.5) + 1):
        if num % i == 0:
            return False
    return True
--- Generated Docstring 4 ---
Return True if num is a prime.

--- Original Code 5 ---
def reverse_string(s):
    return s[::-1]
--- Generated Docstring 5 ---
Reverse a string.

--- Original Code 6 ---
def find_max(data_list):
    if not data_list:
        raise ValueError("List cannot be empty")
    max_val = data_list[0]
    for item in data_list:
        if item > max_val:
            max_val = item
    return max_val
--- Generated Docstring 6 ---
Find the maximum value in a list.

--- Original Code 7 ---
from collections import Counter
def count_words(sentence):
    words = sentence.lower().split()
    return Counter(words)
--- Generated Docstring 7 ---
Count the number of words in a sentence

--- Original Code 8 ---
def fibonacci(n):
    a, b = 0, 1
    for i in range(n):
        yield a
        a, b = b, a + b
--- Generated Docstring 8 ---
Summarize n fibonacci numbers.

--- Original Code 9 ---
def read_file_content(filepath):
    with open(filepath, 'r') as f:
        content = f.read()
    return content
--- Generated Docstring 9 ---
Read the content of a file.

--- Original Code 10 ---
class Circle:
    def __init__(self, radius):
        self.radius = radius
    def calculate_area(self):
        import math
        return math.pi * (self.radius ** 2)
--- Generated Docstring 10 ---
Calculate the area of a circle.

--- Original Code 11 ---
def sum_even_numbers(numbers):
    total = 0
    for num in numbers:
        if num % 2 == 0:
            total += num
    return total
--- Generated Docstring 11 ---
Calculate the sum of even numbers.

--- Original Code 12 ---
def is_palindrome(text):
    cleaned_text = "".join(char.lower() for char in text if char.isalnum())
    return cleaned_text == cleaned_text[::-1]
--- Generated Docstring 12 ---
Return True if text is a palindrome.

--- Original Code 13 ---
def merge_dictionaries(dict1, dict2):
    merged_dict = dict1.copy()
    merged_dict.update(dict2)
    return merged_dict
--- Generated Docstring 13 ---
Merge two dicts.

--- Original Code 14 ---
def celsius_to_fahrenheit(celsius):
    return (celsius * 9/5) + 32
--- Generated Docstring 14 ---
Convert celsius to fahrenheit.

--- Original Code 15 ---
def linear_search(arr, target):
    for i, element in enumerate(arr):
        if element == target:
            return i
    return -1
--- Generated Docstring 15 ---
Return the index of an element in an array.